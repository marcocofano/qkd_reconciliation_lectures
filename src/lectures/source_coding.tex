\chapter{Source coding}

\section{What is Information?}
The word information has, in our minds a meaning that relates to our view of the world as subjects that want to know what is out there. We are recipients of knowledge. Something that is detailed, organized and structured, contains, intuitively, more information that something completely random. A page of your favorite novel, compared with the same page filled with completely random symbols from the same alphabet has more information, isn't it?. 

The meaning of information in information theory, as described by Shannon is quite the opposite. According to Shannon information is the amount of surprise or uncertanty that we experience from an event. With one example, this bizarre way of looking at information, might become more appealing. 
Let's say we have two scenarios: in one city the weather is quite unstable, one day is rainy, the other is sunny.  Let's say there is 50 - 50 probability of rain and sun. There is another city, maybe in the Sahara desert, where the probability of rain drops to zero. 

If we list on the calendar the weather for the first city, it might come as:
\begin{equation}
	RRSRSSSRSRRSR \dots
\end{equation} The calendar looks pretty boring in the Sahara:

\begin{equation}
	SSSSSSSSSSSSS \dots.
\end{equation}

It is pretty clear that if we know that the probabilities are as stated, there is no information content in adding another day in the sahara calendar, if the probability of rain is zero, then it will be S. There is no point in even keeping track of the weather. We will say that the minimum number of information bits that we need to the state of the weather in the Sahara is zero.

So in information theory, information is defined from the point of view of the sender (the Source) and not the receiver.

\section{Information content and Entropy}
Shannon wanted a measure of how much information a source can transmit with a certain amount of unit symbols, or conversely what is the minimum amount of those units we need to convey a certain information. 
How can we measure information content? \newline
The previous example point the main the properties of information:

\begin{enumerate}
	\item  Information is inversely related to the probability of the outcomes
	$$I(x) \sim \frac{1}{P(x)}$$
	\item The information is positive
	$$I(x) \ge 0$$
	\item Information approaches zero if the probability is one 
	$$P(x) = 1 \implies I(x)=0$$
	\item If two independent events are observed, the total information should be the sum of the individual information contents.
	i.e.:
	$$P(x \cap y) = P(x)P(y) \Longrightarrow I(x \cap y) = I(x) + I(y)$$
\end{enumerate}

There is a function that fulfills these 4 properties and Shannon calls it the \textbf{self-information}: 
$$I(x) = \log_B(\frac{1}{P(x)})$$
The above definition is given in units of B-its. 

This function can generalize the information content of an entire distribution. Let's take the expected value of the self information for a random variable X with distribution of probabilities P(X) (in the discrete case of n possible outcomes):
$$H(X) = \sum_{i=1}^ n P(x_i)\log_2\left(\frac{1}{P(x_i)}\right),$$
where in this case we measure the information in bits (base 2).

Shannon (well, John von Neumann) called this quantity, \textbf{Information Entropy} for the random variable X.
The more the distribution differs from the uniform distribution the higher its entropy is. The entropy cannot be negative and reaches zero when the distribution is really just certainty for one outcome.

After this brief informal introduction we will now dive deep into the main results of the source coding problem. If we want to state these results in their mathematical fashion we need to formalize some concepts first.

\section{Symbol Codes}

\begin{definition}[Memoryless Source]
	A memoryless source of the communication is a sequence of i.i.d. random variables $X_1, \dots, X_n$ where $X_i \in \mathcal{X}$. \newline
	The source is called discrete if the random variables are i.i.d. and countable. 
\end{definition}

\begin{definition}[Alphabet]
	An Alphabet $\mathcal{A}$ is a set of values the random variables $X \in \mathcal{X} $ are valued into. A concatenation of alphabet symbols is denoted by $\mathcal{A}^\star$.
\end{definition}

\begin{definition}[Symbol Code]
	A symbol code C is a function
	$$C: \mathcal{X} \longrightarrow \mathcal{A}^\star.$$ \newline
	An extension of a symbol code is an homomorphic map
	$$C^\star: \mathcal{X}^\star \longrightarrow \mathcal{A}^\star, \,\,\, \text{s.t.} \,\,\, C^\star(x_1, \dots, x_n) = c(x_1), \dots, C(x_n)$$ 
\end{definition}

\begin{definition}[Codeword]
	A codeword of C is an element of $A^\star$ that is an image of a certain element of the code function C. 
\end{definition}

\section{Symbol codes - examples}
Let us define the random variable X as
\begin{equation}
	X = \begin{cases}
		a, &P(a) = \frac{1}{2}, \\
		b, &P(b) = \frac{1}{4}, \\
		c, &P(c) = \frac{1}{8}, \\
		d, &P(d) = \frac{1}{8}, \\
	\end{cases}
\end{equation}

And the following three choices for codes with binary alphabet $A=\{0,1\}$ \newline
\newline

\textbf{Code 1}:

\begin{equation}
C(X) = \begin{cases}
C(a) = 0 \\
C(b) = 1 \\
C(c) = 01 \\
C(d) = 10 \\
\end{cases}
\end{equation}

This first code is clearly not useful. There is no way we can uniquely distinguish in a string of received bits what the corresponding variables were. In fact if we receive the string 01 it is impossible to choose between decoding it to $ab$ or $c$, because: $C(ab) = C(a)C(b) = C(c)$. \newline

\textbf{Code 2}: 
\begin{equation}
C(X) = \begin{cases}
C(a) = 101 \\
C(b) = 00 \\
C(c) = 0001 \\
C(d) = 1 \\
\end{cases}
\end{equation}


The code 2 is better, it can be uniquely decoded, but its decoding process is quite inefficient. Let's try and decode the string $(1101000001001)$:

We check the first bit it is consistent with $a$ and $d$. The second bit is not consistent with any bit, so we need to go back and assign the first bit to d. Now we restart from the second. (1) consistent with $a$ and $d$, 10 consistent $a$ but also with $db$ were the next one be a zero. Finally we know it is 101 and we can decode it uniquely to $a$. The rest is similar, we need to go forward to conclude the decoding for a set of bits that are completely in the past (meaning they not include the current). 
The reason why this is happening is that some elements are actually included at the start (they are a prefix) of other elements. \newline

\textbf{Code 3}:

\begin{equation}
C(X) = \begin{cases}
C(a) = 0 \\
C(b) = 10 \\
C(c) = 110 \\
C(d) = 111 \\
\end{cases}
\end{equation} 

The third code fixes the previous two codes' shortcomings. The decoder can analyze each current bit and decide if up to and including that bit we have a new unique decoded codeword or we need a further bit to add. \newline 
These examples introduce some very important feature that our codes need to have. 

\begin{definition}[Uniquely decodable codes]
	A code C is called uniquely decodable iff C* is 1-1.
\end{definition}

\begin{definition}[Prefix codes]
	A code is called prefix (-free) code iff is UD and there is no element that is the prefix of another element. 
\end{definition}

\begin{info}
	Any prefix code is a tree. As an example the code 3 is depicted as a tree in fig. \ref{fig:code3tree} \newline
\end{info}
\begin{figure}[h]
	\begin{center}
		\begin{tikzpicture}
		[
		level 1/.style = {black, sibling distance = 4cm},
		]
		
		\node {1}
		child {
			node {a, 0.5} 
			edge from parent node [left] {0}
		} 
		child { node {0.5}
			node {} 
			child {
				node {b, 0.25} 
				edge from parent node [left] {0}
			}
			child { node {0.25} 
				node {}
				child {
					node {c, 0.125}
					edge from parent node [left] {0}
				}
				child {
					node {d, 0.125}
					edge from parent node [right] {1}
				}
				edge from parent node [right] {1}
			}
			edge from parent node [right] {1}
		};
		
		\end{tikzpicture}
	\end{center}
	\caption{Example code 3 as a tree}
	\label{fig:code3tree}
\end{figure}


\section{Expected codeword length}
In this section we will start laying the foundation of the source coding problem. If we choose a code, how good is it in terms of compression? The code produces for each symbol a different amount of bits. In general it is good if the most probable symbol can be encoded with the minimum amount of bits. The crucial metric for efficiency will then be the average length of message under the code C. \newline\newline
Let $X \sim p$ be a discrete random variable with pmf = $f$, i.e $P(X=x) = p(x)$ with $X_1, \dots, X_n$ iid.
For a sequence $\alpha = (\alpha_1, \dots, \alpha_k)$ where $\alpha_i \in \mathcal{A}^\star$,  we denote $|a|$ the length of $\alpha$. The same notation applies to the lengths of codewords $l(x) = |C(x)|$.

\begin{definition}[Expected length of code]
	The Code C has an expected length of: 
	\begin{equation}	\label{eq:expectedlength}
	L = \sum_{x \in \mathcal{X}}l(x)p(x)
	\end{equation}
\end{definition} 

\begin{example}
	The expected length for the code 3 is:
	$$ L = \frac{1}{2} + 2\frac{1}{4} + 3\frac{1}{8} + 3\frac{1}{8} = 1.75 \, \text{bits}$$
	We will see, that, for this distribution X, this is the best that we can achieve. \newline
Code 2 performs quite badly:
	$$ L = 3\frac{1}{2} + 2\frac{1}{4} + 4\frac{1}{8} + 1\frac{1}{8} = 2.625\, \text{bits}$$
\end{example}

\section{The source coding problem}
We are now in a position to clearly state the problem of efficiency in encoding a message whose symbols probability is known. \newline
\begin{question}
For a certain discrete memoryless random variable X, is there an optimal distribution of lengths for a code C for which the expected codelength is minimal?
\end{question}
To finally address this problem we need to state one of the fundamental theorems that will enable us to crack the source coding problem.
	
\begin{theorem}[Kraft - McMillan]
	The theorem has two parts and it characterize UD codes in terms of their lengths:
	\begin{enumerate}
		\item For any UD B-ary code C
			$$\sum_{x \in \mathcal{X}} \frac{1}{B^{l(x)}} \le 1 \,\,\,\, l(x) = |c(x)|$$
		\item If $l: \mathcal{X} \longrightarrow \{0,1,2,\dots\}$ satisfy:
		$$\sum_{x \in \mathcal{X}} \frac{1}{B^{l(x)}}$$
		Then $\exists$ a B-ary \textbf{prefix} code C with $|C(x)| = l(x)$, $x \in \mathcal{X}$
	\end{enumerate}
\end{theorem}

\begin{example}
	for $B=2$ the code 3 satisfies the KM inequality ($L = 1$, the minimum!) 
\end{example}

\section{The lower bound for the expected length}
We are now in a position to prove the lower bound for L, the expected length of a code C. \newline
We need to minimize the L function in eq. \ref{eq:expectedlength}. It is clear that the minimization only involves the set of length $l(x)$ for the code C. An important requirement is that this minimization is not over any possible code but over prefix UD codes. That is why the KM inequalities are so important. \newline
The problem of source code clearly reduces to the following constrained minimization: 
\begin{equation}
	\min 	L = \sum_{x \in \mathcal{X}}l(x)p(x) \,\,\, \text{subject to} \,\,\, 	\sum_{x \in \mathcal{X}} \frac{1}{B^{l(x)}}\le 1 
\end{equation}

We will solve this minimization problem using the method of Lagrange multipliers, but first we need three main simplifications
\begin{itemize}
	\item The problem is equivalent to the same minimization, but with the constraint inequality substituted with equality.
	\item $l(x_i)$ is a list of integers. We can substitute it with real numbers. The final result will mean that the minimum found will be up to rounding of the resulting $l(x_i)$ to the next integer. 
	\item we change the variable:
	$$q_i = \frac{1}{B^{l_i}}$$
	$$l_i = -\log_B q_i$$
\end{itemize}

The new constrained problem is then:
\begin{equation}
	\min 	L = \sum_{i: q_i > 0}p_i(x)\log\frac{1}{q_i} \,\,\, \text{subject to} \,\,\, 	\sum_i q_i =  1 
\end{equation}

In order to apply the Lagrange multiplier method one needs to check that both the function L is convex (compute the Hessian and check that it is positive definite) and that the constraint is a convex set. The second requirement is easily satisfied:
$$\forall \alpha \in [0,1]\,\,\, \sum_i q_i = 1 \,\,\, \text{and} \sum_i q'_i = 1 \Longrightarrow \sum_i (\alpha q_i (1-\alpha q') = 1) $$

The Langrangian and it minimizing eq. are then:
\begin{eqnarray}
&\mathcal{L} = \sum_{i: q_i > 0}p_i(x)\log\frac{1}{q_i} + \lambda \left(\sum_i q_i - 1\right ) \\
&\nabla_q\mathcal{L} = 0 
\end{eqnarray}
the solution is straightforward
\begin{eqnarray}
&\frac{-p_i}{q_i \ln B} + \lambda = 0 \\
&\frac{p_i}{lnB} = \lambda q_i  \\
&\sum_i\frac{p_i}{\ln B} = \sum_i\lambda q_i \Longrightarrow \lambda = \frac{1}{\ln B} 
\end{eqnarray}
and the minimizing parameters are: $$q_i = p_i\,\, \forall i$$
As we might have expected the function that minimize L is the Information Entropy  H:
\begin{equation}
	H(X) = -\sum_i p_i\log_B p_i
	\label{eq:entropy}
\end{equation}

The set of lengths that minimize L is:
\begin{eqnarray}
&l^\star = argmin_{l: \sum_{i}\frac{1}{B^{l_i}}} \sum_i l_i p_i\\
&= \log_B \frac{1}{p_i}
\end{eqnarray}

\begin{info}
	It is now clear why the code 3 is the optimum. The lengths are log of  reciprocal of the respective probabilities. 
\end{info}

\begin{info}
 The tree graph in figure \ref{fig:code3tree} is not unique for the set of probabilities (1/2,1/4,1/8,1/8). One can use (as Shannon did) some basic properties H needs to have to find that the entropy has the form in eq. \ref{eq:entropy}. One of the fundamental properties is that if a choice is broken down a tree the original H is going to be the weighted average of the individual H for the subtrees. In the example in fig. \ref{fig:code3tree} H has this property: 
 \begin{equation}
	 H(1/2,1/4,1/8,1/8) = H(1/2,1/2) + \frac{1}{2}H(1/4,1/4) + \frac{1}{4}H(1/8,1/8)
	\label{eq:entropybootstrap}	 
\end{equation} 

Notice that this is true for whatever rearranging of this tree. This would produce suboptimal prefix codes with a larger L, but the H will be the same (it is the stationary point for L). The property about the weighted average just pelled out will  hold for any tree that have the same probabilities at the bottom leaves. 
\end{info}

\section{The source coding theorem}
\begin{definition}[Capacity Rate of a channel]
	The capacity $\mathcal{C}$ of a discrete channel is given by:
	\begin{equation}
		\mathcal{C} = \lim_{T\rightarrow \infty} \frac{\log N(T)}{T},
	\end{equation}
	where $N(T)$ is the number of allowed signals of duration T.
\end{definition}
Given a code C, with average message length L, we will encode and send through the channel at a maximum rate $\frac{C}{H}$.
\begin{theorem}
	Given a noiseless channel with channel capacity $\mathcal{C}$ and a iid discrete memoryless source, then it is possible to encode the output of the source in such a way as to transmit at the average rate $\frac{C}{H} -\epsilon$ symbols per second over the channel where $\epsilon$ is an arbitrarily small constant. It is not possible to transmit at an average rate greater than $\frac{C}{H}$
\end{theorem}

\section{Shannon - Fano codes}
The Shannon theorem is a theoretical limit for communication. Even if we can identify the set of length that achieve capacity another (big) problem is to actually find an algorithm to find the correct optimal code with these lengths. Moreover, the entropy for a general probability distribution will not produce an optimal solution with exactly integer lengths. Fano already had introduced codes that would produce codes with good performance and this is acknowledged in \cite{shannon}.
The algorithm codes by Shannon and Fano are easily visualized with the help of trees. 

Let us follow the idea with an example: $$X = (a=0.1,\, b=0.15,\, c=0.2,\, d=0.2,\, e=0.35)$$
Fano thought that the highest probabilities need to be encoded with the fewest possible symbols. So the set of symbols are iteratively divided into two similar probability groups.

\begin{figure}[h]
	\begin{center}
		\begin{tikzpicture}
		[
		level 1/.style = {black, sibling distance = 4cm},
		level 2/.style = {black, sibling distance = 3cm}
		]
		
		\node {1}
		child {
			node {d,e 0.55}
			node {} 
				child {
					node { d 0.2}
					edge from parent node [left] {0}
				}
				child {
					node { e 0.35}
					edge from parent node [right] {1}
				}
			edge from parent node [left] {0}
		} 
		child {
			node {a,b,c 0.45}
			node {}
				child {
					node { c 0.2}
					edge from parent node [left] {0}
				}
				child{
					node { a,b 0.25}
					node {}
						child {
							node { a 0.1}
							edge from parent node [left] {0}
						}
						child {
							node { b 0.15}
							edge from parent node [right] {1}
						}	
					edge from parent node [right] {1}
				}
			edge from parent node [right] {1} 
			};
		
		\end{tikzpicture}
	\end{center}
\end{figure}

The code thus produced is

\begin{equation}
X = \begin{cases}
C(a) = 111\\
C(b) = 110 \\
C(c) = 10 \\
C(d) = 00 \\
C(e) = 01
\end{cases}
\end{equation} 

The entropy is  $H = 2.2016$ bits, the average length achieved is slightly above of H, $L = 2.25$. Even then this code is actually optimal, given that we are dealing with integer lengths. This algorithm does not get it right all the times, though. If we try a more skewed distribution:

$$X = (a=0.15,\, b=0.16,\, c=0.17,\, d=0.17,\, e=0.35)$$

\begin{figure}[h]
	\begin{center}
		\begin{tikzpicture}
		[
		level 1/.style = {black, sibling distance = 4cm},
		level 2/.style = {black, sibling distance = 3cm}
		]
		
		\node {1}
		child {
			node {d,e 0.52}
			node {} 
			child {
				node { d 0.17}
				edge from parent node [left] {0}
			}
			child {
				node { e 0.35}
				edge from parent node [right] {1}
			}
			edge from parent node [left] {0}
		} 
		child {
			node {a,b,c 0.48}
			node {}
			child {
				node { c 0.17}
				edge from parent node [left] {0}
			}
			child{
				node { a,b 0.31}
				node {}
				child {
					node { a 0.15}
					edge from parent node [left] {0}
				}
				child {
					node { b 0.16}
					edge from parent node [right] {1}
				}	
				edge from parent node [right] {1}
			}
			edge from parent node [right] {1} 
		};
		
		\end{tikzpicture}
	\end{center}
\end{figure}

The code thus produced is

\begin{equation}
X = \begin{cases}
C(a) = 110\\
C(b) = 111 \\
C(c) = 10\\
C(d) = 00 \\
C(e) = 01
\end{cases}
\end{equation} 
Which incidentally is the same code as before, but this time is not optimal. The entropy is $H =2.23$, $L=2.31$, but there is a code which achieves L=$2.30$. This could have been discovered separating the $e$ from the rest at the onset. It is crucial that doing this for the fist example would give a suboptimal answer. So Shannon-Fano codes are not a good and unique algorithm to find optimal codes. This very problem actually resisted both Shannon and Fano for 5 years, until Fano started giving a series of lectures to masters students at the university. He half jokingly decided to give this very problem to the student and told them that they could skip the exam if they presented a brief thesis solving this exercise. What he did not tell the student was that this was an open problem that him and Shannon could not solve. Fortunately, a student named David Huffman was in his class and decided he was not going to study for the exam. 

\section{Huffman codes}
Hannon - Fano codes follow a top - bottom approach to build codes. The problem is that the cut to divide the symbol list in two is taken to be such that the sum of the probabilities are roughly the same. This gives non unique answers as we already saw. 

Huffman realized that starting from the bottom could basically enable a unique bootstrap (or iterative) procedure.

Let's take the second example above. 
 $$X = (a=0.15,\, b=0.16,\, c=0.17,\, d=0.17,\, e=0.35)$$
 
The optimal lengths from the Shannon coding theorem are:
 $$l(X)_{\text{min}} = (a=2.73,\, b=2.64,\, c=2.55,\, d=2.55,\, e=0.92)$$

Huffman procedure takes the two lowest probabilities and puts them at the bottom. We don't know yet which degree they are going to end up in. They will certainly meet at a node that we can label with the sum of their probabilities. One can start now from the virtual probability distribution
$$X_1 = (c=0.17,\, d=0.17,\, (a,b)=0.31,\, e=0.35)$$
as if it was our starting point, and considering the sum (a,b) as a new point. This is not the lowest probability anymore. So if we were to start over with Huffman idea that the lowest probability are to be set at the lowest level possible we need to couple c and d together. This is the crucial step missing in Shannon and Fano algorithm. Following the procedure gives
 \begin{figure}[h]
 	\begin{center}
 		\begin{tikzpicture}
 		[
 		level 1/.style = {black, sibling distance = 4cm},
 		level 2/.style = {black, sibling distance = 5cm},
 		level 3/.style = {black, sibling distance = 3cm}
 		]
 		
 		\node {1}
 		child {
 			node {e 0.35}
 			edge from parent node [left] {0}
 		}
 		child {
 				node {0.65}
 				node {}
			child {
 				node {0.34}
 				node {}
 				child {
 					node { c 0.17}
 					edge from parent node [left] {0}
 				}
 				child{
 					node { d 0.17}
 					edge from parent node [right] {1}
 				}
 				edge from parent node [right] {1} 
 			}
 						child {
 				node {0.31}
 				node {}
 				child {
 					node { a 0.15}
 					edge from parent node [left] {0}
 				}
 				child{
 					node { b 0.16}
 					edge from parent node [right] {1}
 				}
 				edge from parent node [right] {1} 
 			}
 		};
 		
 		\end{tikzpicture}
 	\end{center}
 \end{figure}
 
 The code thus produced is
 
 \begin{equation}
 X = \begin{cases}
 C(a) = 110\\
 C(b) = 111 \\
 C(c) = 100\\
 C(d) = 101 \\
 C(e) = 0
 \end{cases}
 \end{equation} 
which has the lowest possible code length $L=2.30$.

There is a n easy way to understand how this is the optimal scheme. First, notice that the depth of the tree is equal to the length of each node at that depth. Second, hat the minimal average length (the entropy) is the sum of all the probabilities in the tree apart from the very root, in the final example:
\begin{eqnarray}
H(p_a,p_b,p_c,p_d,p_e) &=& 3p_a + 3p_b + 3p_c + 3p_d + p_e\\
					   &=& p_c + p_d + p_a + p_b  \,\,\, \text{depth 3}\\
					   &+& (p_c + p_d) + (p_a + p_b) \,\,\, \text{depth 2}\\
					   &+& (p_c + p_d + p_a + p_b) + p_e \,\,\, \text{depth 1}
\end{eqnarray}
We know that the lowest probability nodes minimize L if they go to the lowest possible depth. So we take a and b to be at the bottom but this means separate the previous formula into
\begin{eqnarray}
 H(p_a,p_b,p_c,p_d,p_e) = p_a + p_b + H((p_a+p_b), p_3, p_4, p_5)
\end{eqnarray}
if we separate one instance of $p_a$ and $p_b$, from then on, in the rest of the graph they will always appear as a single entity, i.e. their sum. The reduced problem to find the entropy for the probability $((p_a+p_b), p3, p_4, p_5)$ is solved recursively.


\section{Exercises}
\begin{example}[Exercise 1]
	Find the entropy of a (unfair) coin flip, i.e $H(X)$ where $X \sim \text{Bernoulli(p)}$
\end{example}

\begin{example}[Exercise 2]
	Find the entropy of a, i.e $H(X)$ where $X \sim \text{Bernoulli(p)}$
\end{example}