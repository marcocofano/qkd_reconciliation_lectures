\chapter{Channel coding}
\section{The channel model}

In order to mathematically define the problem of reliable communication of a non perfect channel, we need to model how it behaves and in which way it changes the bits transmitted over it. The main idea is to model the probability that each symbol sent is changed to another symbol of the alphabet. Let us focus on simpler examples. The basic simplifications will be:
\begin{itemize}
	\item memoryless random source: the subsequent bits probabilities are independent and identically distributed (i.i.d). Also the transition probability (noise, error) is independent for each subsequent bit	
	\item symmetry: the transition probability is symmetric under exchange of the bits sent.
\end{itemize}

Although simplified with respect to more general channels, those simple examples can model a wide range of common practical cases encountered in real communication systems and can serve as a foundation for more general cases.

We describe the channel as two random variables, input $X$ and putput $Y, $ and the transition probability, i.e. the conditional probability $P(Y|X)$. In many cases we will consider the prior $P(X)$ to be uniform. 

\subsection*{BEC - Binary Erasure Channel}

The binary erasure channel is described by the model in fig. \ref{fig:becchannel}

\begin{figure}[h]
	\begin{center}
		\begin{tikzpicture}[auto]
		\node [pinstyle] (input) {$X_{t}$};
		\node [pinstyle, right of=input] (inup) {1};
		\node [pinstyle, below of=inup, node distance=2cm] (indown) {-1};
		\node [pinstyle, right of=inup, node distance=4cm] (outup) {1};
		\node [pinstyle, below of=outup, node distance=2cm] (outdown) {-1};
		\node [pinstyle, right of=outup] (output) {$Y_{t}$};
		\node [pinstyle, above of=outdown, node distance=1cm] (erasure) {?};
		

		\draw [->] (inup) -- node[name=u] {$1 - \epsilon$} (outup);
		\draw [->] (inup) -- node[name=a] {$\epsilon$} (erasure);
		\draw [->] (indown) -- node[name=b] {$\epsilon$} (erasure);
		\draw [->] (indown) -- node[name=c] {$1 - \epsilon$} (outdown);
		
		
		\end{tikzpicture}
	\end{center}
	\caption{Binary Erasure Channel}
	\label{fig:becchannel}
\end{figure}

The transition probability is 
\begin{equation}
	P(Y|X) = \begin{cases}
	 P(1|1) = 1 - \epsilon = P(-1|-1) \\
	 P(?|1) = \epsilon = P(?|-1) \\		
	\end{cases}
\end{equation}

\subsection*{BSC - Binary symmetric Channel}
The binary symmetric channel is described by the model in fig. \ref{fig:bscchannel}

\begin{figure}[h]
	\begin{center}
		\begin{tikzpicture}[auto]
		\node [pinstyle] (input) {$X_{t}$};
		\node [pinstyle, right of=input] (inup) {1};
		\node [pinstyle, below of=inup, node distance=1.5cm] (indown) {-1};
		\node [pinstyle, right of=inup, node distance=4cm] (outup) {1};
		\node [pinstyle, below of=outup, node distance=1.5cm] (outdown) {-1};
		\node [pinstyle, right of=outup] (output) {$Y_{t}$};
		
		
		\draw [->] (inup) -- node[name=u] {$1 - \epsilon$} (outup);
		\draw [->] (inup) -- node[name=a] {$\epsilon$} (outdown);
		\draw [->] (indown) -- node[name=b] {$\epsilon$} (outup);
		\draw [->] (indown) -- node[name=c] {$1 - \epsilon$} (outdown);
		
		
		\end{tikzpicture}
	\end{center}
	\caption{Binary Erasure Channel}
	\label{fig:bscchannel}
\end{figure}

The transition probability is 
\begin{equation}
P(Y|X) = \begin{cases}
P(-1|-1) = 1 - \epsilon = P(1|1) \\
P(1|-1) = \epsilon = P(-1|1) \\		
\end{cases}
\end{equation}


\subsection*{(Bi)-AWNG - Binary Input Additive White Gaussian Noise Channel}

The channel is described by the following model:

\begin{figure}[h]
	\begin{center}
		\begin{tikzpicture}[auto]
		\node [pinstyle] (input) {$X_{t}$};
		\node [sum, right of=input, node distance=3cm] (sum) {$+$};
		\node [pinstyle, below of=sum, node distance=1.5cm] (noise) {$Z_t \sim \mathcal{N}(X_t, \sigma^2)$};
		\node [pinstyle, right of=sum, node distance=3cm] (output) {$Y_{t}$};
		
		
		\draw [->] (input) -- node[name=u] {} (sum);
		\draw [->] (noise) -- node[name=u] {} (sum);
		\draw [->] (sum) -- node[name=c] {} (output);
		
		
		\end{tikzpicture}
	\end{center}
	\caption{Binary Erasure Channel}
	\label{fig:biawgnchannel}
\end{figure}

The transition probability is
\begin{equation}
P(Y|X) = \begin{cases}
P(y|-1) = \mathcal{N}(-1, \sigma^2) \\
P(y|1) = \mathcal{N}(1, \sigma^2)		
\end{cases}
\end{equation}


\section{General Codes over Galois fields}
The First Shannon Theorem provides a solution to the problem of optimal compression on lossless communication. This usually means that information and messages are encoded into a digital representation belonging to an algebric structure called a Galois Field. While the theory of Galois field is one of the most fascinating and complex mathematical structure and is related to multiple problems in maths and physics, we will only need very few notions about them. 

In particular for communication theory, the messages the source and receiver exchange will be strings of data, with elements in a field $\mathbb{F}_p$ where p is prime (or a prime power, if this is not the case one cannot construct a field). This field has the following properties:

\begin{itemize}
	\item the number of elements in the field is $|\mathbb{F}_p| = p$ and those elements are the first p integers including 0
	\item the addition map $x + y$  for $x, y \in \mathbb{F}_p$ is the usual modulo-p addition
	\item the multiplication map $xy$  for $x, y \in \mathbb{F}_p$ is the usual modulo-p multiplication
\end{itemize}

the most useful field is $\mathbb{F}_2$ where the elements are (0, 1) with:

\begin{itemize}
	\item the addition map $0 + 0 = 0$, $0 + 1 = 1 + 0 = 1$, $1 + 1 + 0$
	\item the multiplication map $0*0 = 0*1 = 1*0 = 0$, $1*1 = 1$
\end{itemize}


Notice that we can make strings of bits (elements of the field) as messages. The entire set of messages of length n can be equipped with the structure of a vector space $\mathbb{F}^n$ but this space cannot be equipped in general with a inner product (to define the inner product you need some additional requirements). We will use the vector space property in the following sections. 

\begin{definition}[Codes]
	A code C of length $n$ and cardinality M over a field $\mathbb{F}$ is a collection of M elements from $\mathbb{F}^n$, i.e,
	\begin{equation}
	C(n, M) = \{x^{[1]}, \dots, x^{[M]} \}, x^{[m]} \in \mathbb{F}_p, 1\le m\le M.
	\end{equation}
\end{definition}
The elements of the code are called \emph{codewords} and the parameter $n$ is called \emph{codelength}. 

Another mapping of the $\mathbb{F}_2$ is $(0,1) \rightarrow (1, -1)$. We will use one or the other depending on the channel we use. 

\begin{definition}[Code Rate]
	The \emph{rate} of a code $C(n,M)$ is $r=\frac{1}{n}\log_{|\mathbb{F}|}M$. In Shannon terminology it is measured in information symbols per transmitted symbol.
\end{definition}
The number of total elements of $n$ symbols is $|\mathbb{F}|^{n}$. Usually, we take the codewords to be a subset of length equal to a power of the cardinality of the field, i.e. $M = |\mathbb{F}|^{k}$, so that the rate is rational (and we can put some additional structure to the set of codewords). We would then construct the (valid) codewords as all combinations of k symbols of the original length $n$ word. For each k-combination we choose one particular combination of the remaining $n-k$ symbols. The rate is then $R=\frac{k}{n}$

\begin{example}
	Let's take the repetition code where we repeat three times the same element of $\mathbb{F}_2$ as an example. It is a code of type C(3, 2), i.e. $(000, 111)$ whose rate is r=$\frac{1}{3}\log_2 2=\frac{1}{3}$, meaning for each channel use we send 3 bits and only one is actual information.
\end{example}

\begin{definition}[Distance and weight]
	The \emph{weight} of a codeword $x$, $w(x)$ is given by sum of its non-zero symbols. \newline
	Given two elements of a code $x, y \in C(n, M)$, the distance $d$ is given by:
	\begin{equation}
	d(x, y) = w(w-y)
	\end{equation}   
\end{definition}

That this is indeed a distance is easy to prove. We just need to show that it is linear, symmetric, non-negative and fulfills the triangular inequality:
\begin{equation}
d(x, y) \le d(x,z) + d(z, v)
\end{equation}
for any triple, $x, y, z \in \mathbb{F}^n$.

\subsection{Minimal Distance and bounded distance decoder}

\begin{definition}
	The minimum distance of a code C is smallest of all the pairwise distances between all elements of C:
	\begin{equation}
	d(C) = min\{d(x, y): x, y \in C, x \neq y\}
	\end{equation}
\end{definition}


The classical theory of codes was based on finding "good" codes with the largest possible minimum distance. This is due to the fact that if the element of the codes were sufficiently far apart the code itself was more resilient to changes in the symbols due to the noise in the channel, i.e. flipping of symbols would cause the message to go from x to a neighbor vector of $\mathbb{F}^n$ outside the code C and closer to x more than any other vector in C. Let's see this in detail. 

Let $x \in \mathbb{F}^n$ and $t \in \mathbb{N}$. $t$ defines an hypersphere around $x$ which contains all the points of $\mathbb{F}^n$ with distance at most $t$ from $x$. In the binary case t is the number of maximum flips that will bring x to any other vector inside the sphere. If C has a minimum distance of d we can choose t to be
\begin{equation}
t = \left[\frac{d-1}{2}\right],
\label{eq:decodingradius}
\end{equation} 

these spheres will be disjoint (maximum packing for this choice of codewords). This in turn would allow a decoder that can correct exactly at most $t$ errors as shown in fig. \ref{fig:mindist}. This is called the bounded distance decoder.

\begin{figure}[h]
	\textcolor{red}{placeholder for figure mindist}
\end{figure}

\begin{definition}[Bounded distance decoder]
	Given a vector $y$ of a code $C(n,M,d)$ we call decoding radius $t$ as in eq.\ref{eq:decodingradius}. The decoder decides the estimate for $y$ as:
	\begin{equation}
	\hat{x}^{BD}(y) = 
	\begin{cases}
	x \in C, & \text{if } d(x,y) \le t, \\
	error,   & \text{x does not exist}
	\end{cases}
	\end{equation}
\end{definition}

What this definition implies is that the problem of finding "good" codes is the same as choosing the code in such a way that we can pack spheres in a dense way such that we leave as few vectors out of the spheres as possible. 

The \textbf{channel coding problem} is to find the maximum positive code rate below which, for arbitrarily large n, we can still have a probability of error that goes to zero. Is it possible to have this rate bounded away from zero? The answer by Shannon theorem is positive, although the construction of such codes for its proof is not very useful and it is the the very task of classical and modern coding theory.


\subsection{MAP decoding}
We assume we transmit over a channel with a code $C(n,M) = \{x^{[1]}, \dots, x^{[M]}\}$. Every channel is defined as a transition probability $P_{Y|X}(y|x)$. In many applications we will put a uniform prior on the choice of which message to transmit but in the general this is not the case. The source has then a probability of $P_X(x)$. Y is what the receiver observes. It tries to estimate which value of X was sent. The estimate is $\hat{x}(Y) \in C$. The probability that we made an error is $1-p_{X|Y}(\hat{x}(Y)|y)$. Thus, our decoder needs to find the x such that the probability $p_{X|Y}(\hat{x}(Y)|y)$ is maximal:

\begin{equation}
\begin{split}
\hat{x}^{MAP}(y) &= argmax_{x \in C}p_{X|Y}(x|y) \\
&= argmax_{x \in C}p_{Y|X}(y|x)\frac{p_X(x)}{p_Y(y)} \\
\end{split}
\end{equation}
If all codewords are equally likely the Maximum a posteriori decoder (MAP) is equal to the Maximum likelihood decoder (ML) because $p_X(x)=1$.

The formula above allows to completely determine $p(x|y)$. Indeed we know $p(y|x)$, i.e. the channel. Also, we know the prior probability of the source $p(x)$. $p(y)$ is nothing else than the total probability, i.e. $\sum_x p(y|x)p(x)$, which is just a normalization constant to make sure that the formula gives a probability between 0 and 1. 

This is the optimal decoder, and of course it is actually equivalent to the previous one. We will see in the next chapter that unfortunately it is impossible to obtain an implementation (iterative) of this decoder on actual codes. 

\begin{example}[Unreliable communication]
Suppose that the transmitted bits are independent and that $P \{X_t = +1\} = P \{ X_t = -1\} = \frac{1}{2}$. 

We start by considering uncoded transmission over the BSC($\epsilon$). Thus, we send the source bits across the channel as is, without the insertion
of redundant bits. At the receiver we estimate the transmitted bit X based on the observation Y. The decision rule that minimizes the
bit-error probability, call it $\hat{x}^{MAP(y)}$, is to choose that $x \in \{\pm 1\}$ which maximizes $p_{X|Y}(x|y)$ for the given $y$. Since the prior on X is uniform, an application of Bayes
rule shows that this is equivalent to maximizing $p_{Y|X}(y|x)$ for the given $x$. Since
$\epsilon$ < 1, we conclude that the optimal estimator is $\hat{x}^{MAP(y)} = y$.

The probability that the estimate differs from the true value, i.e., $P_b = P \{\hat{x}^{MAP(Y)} \neq X\}$, is equal to $\epsilon$. Since
for every information bit we want to convey we send exactly one bit over the channel
we say that this scheme has rate 1. We conclude that with uncoded transmission we
can achieve a (rate, $P_b$)-pair of (1,$\epsilon$).
\end{example}

\begin{example}[Repetition codes]

If the error probability $\epsilon$ is too high for our application, what transmission strategy can we use to lower it? The simplest strategy is repetition-coding.
Assume we repeat each bit k times. To keep things simple, assume that k is odd. So if X, the bit to be transmitted, has value x then the input to the BSC($\epsilon$) is the k-tuple
$(x, \dots, x)$.

Denote the k associated observations by $Y_1, \dots, Y_k$. It is intuitive, and not hard to prove, that the estimator that minimizes the bit-error probability is given by the
majority rule

\begin{equation}
	\hat{x}^{MAP}(y_1, \dots, y_k) = \text{majority of } \{y_1, \dots, y_k \}.
\end{equation}

Hence the probability of bit error is given by

\begin{equation}
	P_b = P \{\hat{x}^{MAP}(Y) \neq X\}  = P\{\text{at least } \frac{k}{2} \text{ errors occur} \} = \sum_{i>\frac{k}{2}} \binom{k}{i}\epsilon^i (1-\epsilon)^{k-1}
\end{equation}

Since for every information bit we want to convey we send k bits over the channel we say that such a scheme has rate 1 . So with repetition codes we can achieve
the (rate, $P_b$)-pairs ($\frac{1}{k}$, $\sum_{i>\frac{k}{2}}\binom{k}{i}\epsilon^i (1-\epsilon)^{k-1}$). For $P_b$ to approach zero we have to
choose k larger and larger and as a consequence the rate approaches zero as well.
\end{example}


\section{Mutual information}
The channel is defined through a conditional probability, which describes the transition of an input variable to an output one. It is then interesting to extend the notion of entropy to conditional and joint probabilities. This will enable us a fully probabilistic approach to the channel coding problem. This is actually the way in which Shannon solved it in \cite{shannon}. Let us remind first the formula and interpretation of entropy of a distribution.

In the previous chapter we defined the entropy of a distribution $X$ with p.m.f. $P_X(x)$ as (in the discrete case we use a sum, while in the continuous case we switch to an integral)

\begin{equation}
H(X) = -\sum_{x \in X}P_X(x)\log_2\left(P_X(x)\right) = -\mathbb{E}_{P_X}\left[\log_2 P_X\right]
\end{equation}
We have interpreted this quantity as our a priori rate of ignorance about the source information values, before any message is received. So to specify a message of n letters given that the letters' probability distribution is known, I will need $H(p)$ bits per letter on average for large n. After I learned some values of a correlated distribution though, it is as if the distribution is different and the knowledge is slightly better. The improvement is given by the conditional probability $p(x|y)$.
Extending the formula for entropy to conditional probabilities is straightforward

\begin{eqnarray}
H(X|Y)=\sum_y P_Y(y) \left[ - \sum_x P_{X|Y}(x|y) \log
\left(P_{X|Y}(x|y)\right)\right]
= \mathbb{E}_{P_Y} \left[ - \mathbb{E}_{P_{X|Y}} \log P_{X|Y} \right]
\end{eqnarray}
Remember that the conditional probability is defined via the joint probability $P_{XY}(x,y)$
\begin{equation}\label{conditional_entropy}
	P_{X|Y}(x|y) \equiv P_{XY}(x,y)/P_Y(y)
\end{equation}
where $P_Y(y)$ is the marginal computed from the joint probability, i.e. the sum of the joint probability over all the values that $y$ can take.
From this it easy to see that the conditional entropy is just
\begin{equation}
H(X|Y) = H(X,Y) - H(Y)
\end{equation}
We may interpret the conditional entropy as the number of additional bits per letter needed to specify both $x$ and $y$ once $Y$ is known.
Knowing $Y$ then, increases the amount of information we have about $X$. We will need less bits to optimally convey what $X$ is, just by having the knowledge of the related probability distribution $Y$. The measure of this \textbf{information gain} is the \textbf{Mutual Information}.
\begin{eqnarray}
	I(X;Y)=H(X)-H(X|Y).
\end{eqnarray} \label{eq:mutualinfo}
For discrete variables it has the formula
\begin{equation}
	I(X;Y) = \sum_{x,y} P_{XY}(x,y) \log
	{P_{XY}(x,y) \over P_X(x) P_Y(y)}
	= \mathbb{E}_{P_{XY}} \left[\log{P_{XY} \over P_X P_Y}\right]
\end{equation}

\begin{info}[Mutual information and error correction]
	We might have a feel of how this is going to be important for error correction, if we focus a bit more on eq. \ref{eq:mutualinfo}. After knowing $Y$, I would only need "parity rate" = $H(X|Y)$ bits per letter to determine (or communicate) the input instead of n. The difference to what rate of info I would need if I only knew X and this "parity rate" is a rate of bits I can be sure about, their distribution now is certainty, and their entropy is 1. So we can "safely store" the actual information we want to send in "those" bits. The number of bits per letter that are safe to send are clearly less or equal than the mutual information.  
\end{info}
 
It is important to not these properties of the MI:
\begin{enumerate}
	\item It contains all the relations between $X$ and $Y$, unlike the covariance which only gets up to second order ones
	\item It is symmetric in $X$, $Y$
	\item It is a positive quantity. This comes from the obvious calculation which proves that $H(X|Y) = H(X)$ if $X$ and $Y$ are independent.   
	\item It is additive for independent variables. More quantitatively, if $P_{XYWZ}(x,y,w,z) = P_{XY}(x,y) P_{WZ}(w,z)$, then $$I(X,W; Y,Z) = I(X;Y) + I(W;Z)$$
	\item It follows the Data Processing Inequality, i.e. if $X$ and $Y$ have mutual information is I(X,Y) and a third random variable, Z is a (probabilistic) function of Y only, Z cannot have more information about X than Y has about X, $$I(X;Z) \le I(X;Y)
	$$
\end{enumerate}

\begin{example}[BEC channel]
The BEC channel shown in fig.\ref{fig:becchannel} does not admit flips and only erases the inputs with a probability $\epsilon$ the capacity is then described only in terms of this parametes and is:
\begin{equation}
I_{BEC}(\epsilon) = 1-\epsilon
\end{equation}

\begin{proof}
\begin{eqnarray}
	I(X,Y) &=& H(X) - H(X|Y) \\
		   &=& H(X) - \sum_{y}P_Y(y)H(X|Y=y) \\
		   &=& H(X) - P_Y(y=1)H(X|Y=1) \\
		   &-& P_Y(y=?)H(X|Y=?) \\
		   &-& P_Y(y=0)H(X|Y=0) 
\end{eqnarray}
	
First, note that if we know that $Y = \pm 1$ then we are sure of the value of $X$. Then $H(X|y =\pm 1) = H(X)$. Second, from the symmetry of the channel the information that $y=?$ does not influence the information we have on x, in other words if $y=?$ x can still be one of its values with the same proability distribution as before, i.e. $H(X|y=?) = H(X)$. Finally, the probability for $Y=?$ is just $\epsilon$, still by the symmetry of the channel.
The mutual information is then 
\begin{eqnarray}
I(X;Y) &=& H(X) - P_Y(y=?)H(X|Y=?) \\
		&=& (1 -\epsilon)H(X) \\
		&=&  (1 -\epsilon)
\end{eqnarray}
\end{proof}
\end{example}

\begin{example}[Exercise - BSC channel]
Prove that the mutual information of the BSC channel is
	\begin{equation}
		I_{BSC}(p) = 1 - h_b(p)
	\end{equation}
where $h_b(p)$ is the entropy of a fair coin flip.   
\end{example}

\begin{example}[Exercise - Symmetry]
	Prove that the mutual information is symmetric.   
\end{example}




\section{Shannon channel coding theorem}

\subsection{A different take on Entropy - typical strings and the source coding theorem}
Let us take a detour that will prove very useful in what comes next. We take n bits with probability $p(0) = p$ and $p(1) = 1 -p$. We will call a string of those bits \textbf{typical} iff it has exactly $np$ zeros and $n(1-p)$ ones. In the limit of large n, this is the distribution we expect, i.e the probability of a non typical string drops with n. The number of strings that are typical is
\begin{eqnarray}
N_{typ} &=& \frac{n!}{(np)!(n(1-p))!} \\
&\sim& 2^{nH(p)}
\end{eqnarray} 
where we have used Stirling's approximation of the log of the first line and $$H(p) = -p\log(p) -(1-p)\log(1-p)$$ is the entropy of a Bernoulli variable, i.e. a coin flip.
We can rephrase Shannon source coding problem's solution in this form:
\begin{info}[Shannon Source coding theorem revisited]
	To covey all the information Alice wants to send from input $x^n$ chosen from a random variable $X$ we assign a symbol to each typical string. This is done by a $1-1$ map between the typical strings and all the strings of $nH(p)$ bits. This code has about $2^{nH(p)}$ symbols with a priori uniform probability, i.e $P_{typ} = 2^{-nH(p)}$. This means that in the large n the code shortens (compresses) the strings by a factor of $H(p)$. This is a consequence of the fact the probability of an atypical string is negligible in the large $n$ limit.Â¨
\end{info}
\subsection{The noisy channel coding scheme}
In the following sections we will broadly follow \cite{wilde_2013}, on chapters 3 and 14, to sketch the main ideas on Shannon channel coding problem. 
First, let us remind the general set up of a noisy channel communication. Alice selects the message $m$ and encodes it into $$x^n(m).$$ 
The length of the encoded message is now typically greater than m itself. Bob receives $$y^n(m).$$
The rate of the code is
$$R = \frac{1}{n}\log(M)$$
where $M$ is the number of possible messages. Bob receives through a noisy channel denoted by its conditional probability $P_{Y|X}(y|x)$, a string $y^n$. In general this string is a random variable $Y$ with probability $P_Y(y)$, i.e. not knowing the actual input. This probability is: $$P_Y(y) = \sum_{x \in X}P_{Y|X}(y|x)P_X(x)$$.

The probability that Bob decodes the message wrongly is denoted by the error probability 
\begin{equation}
P^e(m, C),
\end{equation}
where C is the code chosen and m is the input message. The average over all possible messages and the max probability of error are the crucial quantities:
\begin{eqnarray}
\overline{P}^{e}_n(m, C) &=& \frac{1}{M}\sum_{m=1}^{M}P^e_n(m,C) \\
\overline{P}^{*e}_n(C) &=& \underset{m \in |M|}{\max}P^e_n(m,C) \\
\end{eqnarray} 
Shannon's noisy channel problem can be then stated as follows:
\begin{info}[Shannon's noisy channel problem]
	Is there a code for which Bob can estimate $\hat{x}^n$ so that the average probability of error vanishes in the large n limit, for a maximal finite R. 
\end{info}
The fact that the average probability of error depends on the message and the code at the same time is the crucial difficulty in solving this problem.
Shannon solves this first step in a brilliant way.
\subsection{Random codes}
In general the message Bob receives once it is encoded by Alice, is deterministic. We actually send a specific message and the probability of error should depend on the message sent. The brilliant idea from Shannon was to choose the coding scheme at random. If Alice selects the coding scheme from a uniform distribution of possible codes the message will be encoded to $x^n$ at random from this uniform distribution $X$. We can then analyze the expected probability of error with respect to the chosen C (averaging on the codes) instead of averaging on the messages. The probability of choosing a random code then makes no explicit reference to the message being encoded and if we take the expectation value of the average probability of error, this quantity is independent of the message being sent:
\begin{eqnarray}
\mathbb{E}_{\mathcal{C}}\{\overline{P}^e(\mathcal{C})\} &=& \mathbb{E}_{\mathcal{C}}\left\{\frac{1}{M}\sum_{m=1}^{M}P^e(m,\mathcal{C})\right\} \\
 &=& \frac{1}{M}\sum_{m=1}^{M}\mathbb{E}_{\mathcal{C}}P^e(m,\mathcal{C}).
\end{eqnarray}
As stated above the expectation of the probability of error does not depend on the message explicitly, so we can choose whatever message, let's say $1$. 
\begin{equation}
\mathbb{E}_{\mathcal{C}}\{\overline{P}^e(\mathcal{C})\} = \mathbb{E}_{\mathcal{C}}\{P^e(1,\mathcal{C})\}.
\end{equation}
Notice that we do not need to average over all the messages now.


\subsection{Shannon channel coding theorem}
We construct a whole family of codes with increasing length n and the number of all possible messages Alice sends is M. The code rate is $R = \frac{\log M}{n}$. The number of different (valid) codewords messages is then $$M = 2^{nR}.$$  As we saw, we want to maximize the distance between valid codewords. This is so because we want to determine what is the number of messages that are distinguishable after running through the channel. 

Alice chooses the codewords from $X$. So, the codewords she chooses are with exponentially increasing probability, typical for that distribution, whose entropy is close to $H(X)$. Bob receives a string from $Y^n$ that depends on the original message through the conditional probability $P_{Y|X}(y|x)$. What are the possible output sequences, out of all of them, that are likely to correspond to a particular $x^n$? The probability $P_{Y|X}(y|x)$ has its own entropy $H(Y|X)$ and more importantly its own definition of typicality. Those sequences are still $2^{nH(Y|X)}$ in total and they are the one typically received by Bob, if $x^n$ was sent by Alice. We can then partition the set of all possible typical sequences of the distribution of received sequences $Y_n$, which are $2^{nH(Y)}$ into groups containing typical sequences depending on the sent distribution $X$. Bob needs to be able to know if a received codeword fits into only one of these groups. To be sure that there exists such a scheme we need to make sure that there are no overlaps between the groups. Given that all possible messages are $2^{nR}$ then:
\begin{eqnarray}
2^{nR} = \frac{2^{nH(Y)}}{2^{nH(Y|X)}} = 2^{n(H(Y) - H(Y|X))}
\end{eqnarray}
so the message can be reliably decoded, i.e. every code bob receives is associated uniquely with a possible typical input sequence when:
\begin{eqnarray}
R < H(Y) - H(Y|X) = I(Y;X)
\end{eqnarray}
It seems that the best can be reached at the maximum over all possible inputs probability distributions, which is called the \textbf{Channel Capacity} (This is actually possible due to the fact that the mutual information $I$ is concave).
\begin{equation}
C_{shannon} = \max_{\{p(x)\}} I(X;Y)
\end{equation}

The problem is then, can we actually attain a rate R such that the probability of error $P_n \rightarrow 0$ for $n \rightarrow 0$ and R arbitrarily close to C?

There are three possible errors that can occur:
\begin{itemize}
	\item The channel output $y^n$ is a codeword that is not typical according to $Y$
	\item the channel output $y^n$ is typical for $Y$, but is not in the set of conditionally typical sequences for random variable associated with the probability $P_{Y^n|X=x^n(m)}(y|x^n(m))$ also known as the conditionally 
	$\delta$-typical set $T^{Y^n|x^n(m)}_\delta$ (the $\delta$ here is a technical condition on the distribution to which $y^n$ belong being arbitrarily close to the true one, so $\delta$ can be chosen arbitrarily small)
	\item The channel output $y^n$ is in $T^{Y^n|x^n(m^\prime)}_\delta$ but for the wrong message $m^\prime \neq m$.
\end{itemize}

It is easy to understand what being in the conditionally typical set means for a BSC. The channel will alter the codeword sent flipping the bits. The flipping is a string of bits of the same length of the original codeword. This is with high probability a typical string. So the total output (conditionally typical strings) will be the ones that are a combination (sum mod2) of the input plus the flipping.  

Let's focus on the last error type, only because it is the most important one. This is not a full proof of the theorem but only a sketch of how to proceed. For a full proof check \cite{wilde_2013} chapter $14$, where the reader will also find a thorough exposition on typicality and typical sets. 

Around $y^n$ there is a sphere filled with conditionally typical strings that contains a codeword $x^n(m)$. Can there be another one? The number of conditionally typical strings around $y^n$ is $2^{nH(Y|X + \delta}$ where we included a possible small departure from typicality. 
The fraction of these strings in the sphere over all possible typical received strings is:
\begin{equation}
\frac{2^{n(H(Y|X) + \delta)}}{2^{n(H(Y) + \delta)}}
\end{equation}

Since the codewords are chosen at random uniformly, the probability that this sphere contains any particular other codeword is the fraction above multiplied by the number of all codewords, which is  $2^{nR}$. In other words, this is the probability that any element inside the sphere is another codeword (apart from the one that we know with almost certainty given that it is a typical set), i.e.  the probability of an element being in the sphere times the probability of it being a codeword. 

So the average probability of error 
\begin{eqnarray}
\mathbb{E}_{\mathcal{C}}\{\overline{P}^e(\mathcal{C})\} =
2^{-n(H(Y) - H(Y|X) - R - 2\delta)} = 2^{-n(I(Y;X) - R - 2\delta)}
\end{eqnarray}
which goes to zero ar n goes to infinity, for a fixed R ($\delta$ can be made arbitrarily small).
 
What we have given  in this section is a sketch of the actual proof of the very important theorem:

\begin{theorem}[Shannon Channel Coding theorem]
	There exists a constant C called the Channel Capacity, depending on the transition probability of the channel, such that if the rate is such that $0 \le r \le C(\alpha)$ then:
	\begin{equation}
		\hat{P}^{MAP}_B(n, 2^{[rn]}, \alpha) \rightarrow 0 \,for \, n \rightarrow 0.
	\end{equation} 
\end{theorem}

The converse is also true, namely that if one attempts to decode with a code of rate greater that the Shannon Capacity it will incur in probabilities of error that are bounded away from zero (one can show that for those codes the block error probability goes to one for any sequence of increasing block length n).

\begin{info}[Classical coding theory vs error correction codes (Modern coding theory)] % Information block
	 Shannon's result is an incredible achievement and a very profound one in the understanding of information content and proving that we can actually transmit information through the channel and recover a part of it. Again it uses an probabilistic ensamble sampling method. Even if it is quite interesting that a random code performs in such a way that the capacity can be attained, it is not a clever way of actually coming up with good codes. This is because the proof tells us that we can achieve capacity in the limit of large n and for that the actual encoders and decoders would need to become larger and larger as well.We will see that devising good codes, analyzing performance and even the very process of decoding is far more challenging and interesting. The key to attacking the problem of building good codes is to impose some algebraic structure to them that can be exploited to get as close as possible to the optimum dictated by Shannon's Theorem (high rate) and still get a possibly vanishing error rate, without using exceedingly long codes.
\end{info}




